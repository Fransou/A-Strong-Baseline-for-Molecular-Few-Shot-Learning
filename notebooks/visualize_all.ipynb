{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc6940d",
   "metadata": {},
   "source": [
    "# Plotting Few-Shot Model Evaluation Results\n",
    "\n",
    "Assembling plots from summary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648ec364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Setting up local details:\n",
    "# This should be the location of the checkout of the FS-Mol repository:\n",
    "FS_MOL_CHECKOUT_PATH = os.path.join(os.environ['HOME'], \"fsl/git_repos/few_shot_drug\")\n",
    "FS_MOL_DATASET_PATH = os.path.join(os.environ['HOME'], \"fsl/git_repos/datasets/few_shot_drug\")\n",
    "\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)\n",
    "PAPER_FIGDIR = \"../paper/Paper/fig\"\n",
    "from fs_mol.plotting.utils import (\n",
    "    highlight_max_all, \n",
    "    #plot_all_assays, \n",
    "    load_data,\n",
    "    expand_values,\n",
    "    #plot_task_performances_by_id,\n",
    "    #box_plot,\n",
    "    #plot_by_size,\n",
    "    get_aggregates_across_sizes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd98c9",
   "metadata": {},
   "source": [
    "## Making summary files\n",
    "\n",
    "Summary files are obtained by running `fs_mol/plotting/collect_eval_runs.py` on the outputs of evaluation runs. If an evaluation output directory is \"evaluation_output_directory\" then summary files are created with:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81bb9e",
   "metadata": {},
   "source": [
    "The option `--plot` results in a plot across support set sizes for each few-shot testing task. Final summarized results will be found in \"evaluation_output_directory/summary/{model_name}_summary.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e4c16c",
   "metadata": {},
   "source": [
    "## Loading the collated evaluation data\n",
    "\n",
    "Create a dictionary of all model summary .csvs to be compared. The csvs are the final summaries from `collect_eval_runs.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d28293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure this to contain all the models that you want to look at.\n",
    "# Dict keys are human readable names, values are the path to the summary produced by collect_eval_runs.py\n",
    "\n",
    "results_path = os.path.join(\"/home/philippe/fsl/git_repos/baselines\")\n",
    "\n",
    "# a dictionary summarising all models to be compared. Add new paths here as desired.\n",
    "model_summaries = {\n",
    "    \"ADKF-IFT\": os.path.join(results_path, \"ADKF-IFT_classification_summary.csv\"),\n",
    "    \"Q-probe\": os.path.join(results_path, \"qprobe_summary.csv\"),\n",
    "    \"L-probe\": os.path.join(results_path, \"lp_summary.csv\"),\n",
    "    \"PN\": os.path.join(results_path, \"ProtoNet-gnn+ecfp+fc-Support64_summary.csv\"),\n",
    "    \"mhnfs\" : os.path.join(results_path, \"mhnfs_summary.csv\"),\n",
    "    \"clamp\": os.path.join(results_path, \"clamp_summary.csv\"),\n",
    "    \"CNP\": os.path.join(results_path, \"CNP_classification_summary.csv\"),\n",
    "    \"GNN-MAML\": os.path.join(results_path, \"MAML-Support16_summary.csv\"),\n",
    "    \"SimSearch\": os.path.join(results_path, \"SimSearch_summary.csv\"),\n",
    "    \"PAR\": os.path.join(results_path, \"PAR_classification_summary.csv\"),\n",
    "    \"GNN-MT\": os.path.join(results_path, \"GNN-Multitask_summary.csv\"),\n",
    "    \"MAT\": os.path.join(results_path, \"MAT_summary.csv\"),\n",
    "    \"GNN-ST\": os.path.join(results_path, \"GNN-ST_summary.csv\"),\n",
    "    \"RF\": os.path.join(results_path, \"random_forest_summary.csv\"),\n",
    "    \"kNN\": os.path.join(results_path, \"kNN_summary.csv\"),\n",
    "}\n",
    "# Generated plots will be stored here, if you want to keep them. None disables saving.\n",
    "\n",
    "plot_output_dir = os.path.join(results_path, \"plots\")\n",
    "all_tasks_output_dir = os.path.join(results_path, \"plots/all_tasks\")\n",
    "os.makedirs(plot_output_dir, exist_ok=True)\n",
    "os.makedirs(all_tasks_output_dir, exist_ok=True)\n",
    "\n",
    "data = load_data(model_summaries)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bbcbc-8fa2-4c1f-b8b8-13aac15d952f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0afabd3f",
   "metadata": {},
   "source": [
    "## Highlight the best result for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a47a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand out from val +/- error format, and calculate delta AUPRC\n",
    "data = expand_values(data, model_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ec0b2",
   "metadata": {},
   "source": [
    "### Incorporate protein information\n",
    "\n",
    "Our test tasks have associated target protein information available. We can merge this data to allow plotting with specific EC number classes highlighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01076514",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_path = os.path.join(FS_MOL_CHECKOUT_PATH, \"datasets/targets\", \"test_proteins.csv\")\n",
    "ecs = pd.read_csv(protein_path)\n",
    "ecs[\"target_id\"] = ecs[\"target_id\"].astype(int).astype(str)\n",
    "ecs[\"chembl_id\"] = ecs[\"chembl_id\"].astype(str)\n",
    "ecs[\"TASK_ID\"] = ecs.apply(lambda row: row[\"chembl_id\"][6:], axis = 1)\n",
    "data = ecs.merge(data, on=\"TASK_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46adcfcc",
   "metadata": {},
   "source": [
    "## Aggregate as a function of the number of training points, across all categories\n",
    "\n",
    "Here the results are aggregated according to EC class, and across all classes. This is used to plot the variation of performance with support set size, comparing all models in the model_summaries dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_df = get_aggregates_across_sizes(data, model_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415188aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results of Table 2 for support set size 16\n",
    "aggregate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ee4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function has the option to plot all classes separately.\n",
    "#autoreload magic line\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from fs_mol.plotting.utils import plot_by_size\n",
    "fig=plot_by_size(aggregate_df, model_summaries, plot_all_classes = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bccd0b9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(columns=[\"model\", \"support_set_size\", \"delta-auprc\", \"EC_category\"])\n",
    "for col in aggregate_df.columns:\n",
    "    if not \"std\" in col:\n",
    "        supp_size = int(col.split(\"_\")[0])\n",
    "        model_name = col.split(\"(\")[1][:-1]\n",
    "        for index in aggregate_df.index:\n",
    "            df.loc[df.size] = {\"model\": model_name, \"support_set_size\": supp_size, \"delta-auprc\": aggregate_df[col][index], \"EC_category\":index}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac30dd",
   "metadata": {},
   "source": [
    "# Ranking\n",
    "\n",
    "Here we use [autorank](https://pypi.org/project/autorank/) for an appropriate comparison between all methods when evaluated on multiple tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autorank import autorank\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# select correct data to rank with autorank\n",
    "df_result = pd.DataFrame()\n",
    "for size in [16,32,64,128]:\n",
    "    df = data[[x for x in list(data.columns) if x.startswith(f\"{size}\") and \"val\" in x and \"delta-auprc\" in x]]\n",
    "    ranked_df = autorank(df, alpha=0.1, verbose=False).rankdf\n",
    "    result = pd.DataFrame(ranked_df.meanrank).reset_index(names=[\"model\"])\n",
    "    result[\"model\"] = result[\"model\"].apply(lambda x: x.split(\"(\")[1].split(\")\")[0].replace(\" \", \"\"))\n",
    "    result[\"size\"] = size\n",
    "    df_result = pd.concat([df_result, result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ft_palette=sns.cubehelix_palette(start=0., rot=0.6, dark=0.45, light=.8, as_cmap=True, reverse=True, hue=1.)\n",
    "meta_palette=sns.cubehelix_palette(start=1.7, rot=0.6, dark=0.45, light=.8, as_cmap=True, reverse=True, hue=1.)\n",
    "baseline_palette=sns.cubehelix_palette(start=1.3, rot=0.2, dark=0.6, light=.8, as_cmap=True, reverse=True)\n",
    "\n",
    "ft_palette"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "meta_palette"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_palette"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(6, 7))\n",
    "sns.set_style(\"white\")\n",
    "fine_tuning_methods = [\"GNN-MT\",\"clamp\", \"L-probe\", \"Q-probe\", \"MAT\"]\n",
    "meta_methods = [\"PAR\", \"GNN-MAML\",\"PN\",\"ADKF-IFT\", \"CNP\", \"mhnfs\"]\n",
    "bsl_methods = [\"SimSearch\", \"RF\",]\n",
    "\n",
    "df_result[\"type\"] = df_result[\"model\"].apply(lambda x: \"fine-tuning\" if x in fine_tuning_methods else (\"meta-learning\" if x in meta_methods else \"baselines\"))\n",
    "DOT_SIZE = 300\n",
    "LINE_WIDTH = 3\n",
    "alpha = 0.7\n",
    "\n",
    "method_palette = [baseline_palette, meta_palette, ft_palette]\n",
    "\n",
    "markers_all_methods = {\n",
    "    \"GNN-MT\": \"d\",\n",
    "    \"clamp\": \"8\",\n",
    "    \"L-probe\": \"X\",\n",
    "    \"Q-probe\": \"*\",\n",
    "    \"MAT\": \"<\",\n",
    "    \"PAR\": \">\",\n",
    "    \"GNN-MAML\": \"D\",\n",
    "    \"PN\": \"P\",\n",
    "    \"ADKF-IFT\": \"h\",\n",
    "    \"mhnfs\": \"H\",\n",
    "    \"CNP\": \"s\",\n",
    "    \"SimSearch\": \"p\",\n",
    "    \"RF\": \"o\",\n",
    "}\n",
    "\n",
    "\n",
    "for methods,palette in zip([bsl_methods, meta_methods, fine_tuning_methods, ], method_palette):\n",
    "    # Plot lineplot for each model with x=size, y=mearank and the hue corresponds to the average meanrank of the model\n",
    "    df = df_result[df_result.model.isin(methods)].sort_values(\"meanrank\")\n",
    "    average_meanrank = df.groupby(\"model\").meanrank.mean().reset_index().rename(columns={\"meanrank\": \"mean_meanrank\"})\n",
    "    df = df.join(average_meanrank.set_index(\"model\"), on=\"model\", how=\"outer\").sort_values(\"mean_meanrank\")\n",
    "\n",
    "    sns.lineplot(x=\"size\", y=\"meanrank\", hue=\"mean_meanrank\", data=df, alpha=alpha, legend=False, ax=ax, palette=palette, linewidth = LINE_WIDTH, estimator=None, units=\"model\")\n",
    "    sns.scatterplot(x=\"size\", y=\"meanrank\", style=\"model\", data=df,ax=ax, s=DOT_SIZE, hue = \"mean_meanrank\", palette=palette, markers=markers_all_methods, legend=False)\n",
    "\n",
    "\n",
    "# Custom legend with all markers\n",
    "# place in the legend if the method is meta-learning, fine-tuning or baseline\n",
    "legend_elements = []\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "first_of_each = {\n",
    "    \"fine-tuning\": True,\n",
    "    \"meta-learning\": True,\n",
    "    \"baselines\": True,\n",
    "}\n",
    "for model, marker in markers_all_methods.items():\n",
    "    type = \"\"\n",
    "    if model in fine_tuning_methods:\n",
    "        color = ft_palette(0)\n",
    "        type = \"fine-tuning\"\n",
    "    elif model in meta_methods:\n",
    "        color = meta_palette(0)\n",
    "        type = \"meta-learning\"\n",
    "    else:\n",
    "        color = baseline_palette(0)\n",
    "        type = \"baselines\"\n",
    "    if first_of_each[type]:\n",
    "        #legend_elements.append(Line2D([], [], marker=\"\", color=\"w\", label=type.upper(), markersize=0, linestyle='None'))\n",
    "        first_of_each[type] = False\n",
    "    legend_elements.append(Line2D([0], [0], marker=marker, color=color, label=model, markersize=15, linestyle='None'))\n",
    "ax.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., prop={'size': 15})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# place in the legend if the method is meta-learning, fine-tuning or baseline\n",
    "#ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "\n",
    "#Move legend to the right\n",
    "#ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "ax.set_xlabel(\"$|\\mathcal{S}|$\")\n",
    "ax.set_ylabel(\"Average rank\")\n",
    "\n",
    "ax.set_xticks([16,32,64,128])\n",
    "ax.invert_yaxis()\n",
    "plt.grid(True)\n",
    "plt.savefig(PAPER_FIGDIR+\"/ranking.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef357b19",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame()\n",
    "for size in [16,32,64,128]:\n",
    "    df = data[[x for x in list(data.columns) if x.startswith(f\"{size}\") and \"val\" in x and \"delta-auprc\" in x]]\n",
    "    result = pd.DataFrame(autorank(df, verbose=False).rankdf.meanrank).reset_index(names=[\"model\"])\n",
    "    result[\"model\"] = result[\"model\"].apply(lambda x: x.split(\"(\")[1].split(\")\")[0].replace(\" \", \"\"))\n",
    "    result[f\"meanrank_{size}\"] = result[\"meanrank\"]\n",
    "    result = result.drop(columns=[\"meanrank\"])\n",
    "    if len(df_result)==0:\n",
    "        df_result = result\n",
    "    else:\n",
    "        df_result = df_result.join(result.set_index(\"model\"), on=\"model\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119191ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6be497",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Styled df for latx paper, with precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cce00b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "aggregate_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91c2cdb63b030871da94aa046e171a8c212268d3e9d71e3496f8c89eaedb0da0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
