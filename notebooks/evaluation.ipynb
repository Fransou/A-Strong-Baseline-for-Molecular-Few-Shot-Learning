{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models on FS-Mol\n",
    "\n",
    "`fs_mol.utils.test_utils.eval_model()` is a utility function that allows to evaluate a model against the full set of FS-Mol testing tasks, following a fixed evaluation scheme.\n",
    "\n",
    "It requires the following arguments:\n",
    "* `test_model_fn: Callable[[FSMolTaskSample, str, int], BinaryEvalMetrics]`:\n",
    "  This is the core evaluation function, taking a `FSMolTaskSample` (that is, a sample of datapoints from a single task -- see notebooks/dataset.ipynb), a temporary output folder in which to store scratch data, and the seed used for the sampling.\n",
    "  It should return `BinaryEvalMetric` object containing all metrics calculated from the model output and labels of the task sample.\n",
    "* `dataset: FSMolDataset`:\n",
    "  The actual dataset, which can be the full FS-Mol dataset as released, or a different set created directly through its constructor.\n",
    "  This is particularly useful if you are planning to evaluate models on a set of tasks differing from FS-Mol's test set.\n",
    "\n",
    "The returned results contain a list of evaluation metrics for each task, in a dictionary indexed by task name.\n",
    "\n",
    "Additionally, it can be optionally further configured using the following optional arguments:\n",
    "* `out_dir: Optional[str]`:\n",
    "   If provided, a summary of the evaluation results will be written, as one CSV file per task.\n",
    "* `seed: int`:\n",
    "   A seed value used to make sampling and splitting of tasks deterministic.\n",
    "   If set to anything but `0`, results of `eval_model` will be incomparable to the canonical evaluation runs.\n",
    "* `num_samples: int`:\n",
    "   The number of random splits to draw for the task undergoing evaluation.\n",
    "* `train_set_sample_sizes: List[int]`:\n",
    "   The sizes of training / support set data to consider. For each of these, `num_samples` samples will be drawn for each test task, and then be used to call `test_model_fn` with, where the passed `FSMolTaskSample` will have the requested number of `train_samples`.\n",
    "* `valid_size_or_ratio: Union[int, float]`:\n",
    "   Number or ratio of `train_samples` in the drawn samples that will be split out into `validation_samples`. This is useful for models that require an explicit validation set during fine-tuning.\n",
    "* `test_size_or_ratio: Optional[Union[int, float, Tuple[int, int]]]`:\n",
    "   Number or ratio of `test_samples` that will be drawn from the original task. By default, all available samples will be used, but it may be useful to limit this when not using `eval_model` for full model evaluation.\n",
    "* `fold: DataFold`:\n",
    "   The fold of FS-Mol on which to perform evaluation, typically will be the test fold.\n",
    "* `task_reader_fn: Optional[Callable[[List[RichPath], int], Iterable[FSMolTask]]]`:\n",
    "   Callable allowing additional transformations on the data prior to its batching and passing through a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Evaluating a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up local details:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# This should be the location of the checkout of the FS-Mol repository:\n",
    "# This should be the location of the checkout of the FS-Mol repository:\n",
    "FS_MOL_CHECKOUT_PATH = os.path.join(os.environ['HOME'], \"fsl/few_shot_drug\",)\n",
    "FS_MOL_DATASET_PATH = os.path.join(os.environ['HOME'], \"Desktop/fsmol\", \"FS-Mol/datasets/fs-mol\")\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a simple implementation of a `test_model_fn` that creates a random forest model, using the scikit-learn default implementation and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these functions, we can now evaluate these models on the test tasks in FS-Mol as follows, reducing the space of considered samples from each task for speed purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from molfeat.trans import MoleculeTransformer\n",
    "from molfeat.trans.fp import FPVecTransformer\n",
    "# sanitize and standardize your molecules if needed\n",
    "transformer = FPVecTransformer(kind='rdkit', dtype=float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datamol as dm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.data.fsmol_dataset import FSMolDataset, DataFold\n",
    "from fs_mol.utils.test_utils import eval_model\n",
    "from tqdm.notebook import tqdm\n",
    "fsmol_dataset = FSMolDataset.from_directory(FS_MOL_DATASET_PATH)\n",
    "p_bar = tqdm(total=157)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from fs_mol.data.fsmol_task import FSMolTaskSample\n",
    "from fs_mol.utils.metrics import BinaryEvalMetrics, compute_binary_task_metrics\n",
    "\n",
    "def test_model_fn(\n",
    "    task_sample: FSMolTaskSample, temp_out_folder: str, seed: int\n",
    ") -> BinaryEvalMetrics:\n",
    "    train_data = task_sample.train_samples\n",
    "    test_data = task_sample.test_samples\n",
    "\n",
    "    train_smiles = [x.smiles for x in train_data]\n",
    "    test_smiles = [x.smiles for x in test_data]\n",
    "\n",
    "    X_train= np.array([x.descriptors for x in train_data])\n",
    "    X_test = np.array([x.descriptors for x in test_data])\n",
    "\n",
    "    y_train = [float(x.bool_label) for x in train_data]\n",
    "    y_test = [float(x.bool_label) for x in test_data]\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute test results:\n",
    "    y_predicted_true_probs = model.predict_proba(X_test)[:, 1]\n",
    "    test_metrics = compute_binary_task_metrics(y_predicted_true_probs, y_test)\n",
    "    p_bar.update(1)\n",
    "    return test_metrics\n",
    "\n",
    "results = eval_model(\n",
    "    test_model_fn=test_model_fn,\n",
    "    dataset=fsmol_dataset,\n",
    "    # Restrict number of samples to one per task:\n",
    "    train_set_sample_sizes=[16],\n",
    "    num_samples=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import asdict\n",
    "\n",
    "results = {k: asdict(v[0]) for k, v in results.items()}\n",
    "results = pd.DataFrame(dict(results)).T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results[\"d_aucpr\"] = (results.avg_precision - results.fraction_pos_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results[\"method\"] = \"rdkit_desc\"\n",
    "\n",
    "results_all = pd.concat([results_all, results])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_all"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(data=results_all, y=\"d_aucpr\", x=\"method\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_all[results_all.d_aucpr<0].iloc[:,:-2].drop_duplicates(subset=\"task_name\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task_iter = fsmol_dataset.get_task_reading_iterable(DataFold.TEST)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for task in iter(task_iter):\n",
    "    if task.name in results_all[results_all.d_aucpr<0].iloc[:,:-2].task_name:\n",
    "        smiles = [x.smiles for x in task.samples]\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "smiles"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(smiles, columns=[\"smiles\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"label\"] = [x.bool_label for x in task.samples]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "descriptors = np.array([x.descriptors for x in task.samples])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[[f\"desc_{i}\" for i in range(descriptors.shape[1])]] = descriptors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.kdeplot(data=df, x=\"desc_3\", hue=\"label\", alpha=.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91c2cdb63b030871da94aa046e171a8c212268d3e9d71e3496f8c89eaedb0da0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
