{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify, train and evaluate a PyTorch-based model using AbstractTorchFSMolModel\n",
    "\n",
    "We provide framework code so that few-shot models can be evaluated according to our benchmarking procedure using the `AbstractTorchFSMolModel` base class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "2363-2048"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up local details:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# This should be the location of the checkout of the FS-Mol repository:\n",
    "FS_MOL_CHECKOUT_PATH = os.path.join(os.environ['HOME'], \"Projects\", \"FS-Mol\")\n",
    "FS_MOL_DATASET_PATH = os.path.join(os.environ['HOME'], \"Datasets\", \"FS-Mol\")\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a new `AbstractTorchFSMolModel`\n",
    "\n",
    "As example, we use `fs_models/models/gnn_multitask.py`, our implementation of a GNN model implementation, but repeat function definitions integrating with the larger framework in simplified form in this notebook.\n",
    "\n",
    "We need to implement five methods required by the `AbstractTorchFSMolModel` class:\n",
    "\n",
    "* `forward(self, batch: BatchFeaturesType) -> BatchOutputType`:\n",
    "  This has to implement the core model, as usual.\n",
    "  In the GNNMultittask setting, it consumes a `FSMolMultitaskBatch` object, which is created by our batching pipeline, and extends the standard `FSMolBatch` by a mapping from each graph in the batch to a unique task ID.\n",
    "   \n",
    "  As is standard, our `GNNMultitaskModel` creates all needed sublayers in `__init__` and we now simply plug these together.\n",
    "  Concretely, we simply chain four components:\n",
    "  * Use `model.init_node_proj` to project the node features to the input for the GNN.\n",
    "  * Use `model.gnn` to implement the graph-based message passing.\n",
    "  * Use `model.readout` to compute a graph-level representation for all graphs in a minibatch.\n",
    "  * Use `model.tail_mlp` to compute per-task predictions for each graph in the minibatch.\n",
    "\n",
    "  The output of this method should be a `TorchFSMolModelOutput` object, which at least contains the predictions that can be used to calculate a differentiable loss when combined with graph labels from the batch.\n",
    "\n",
    "* `get_model_state(self) -> ModelStateType` and `load_model_state(self, model_state: ModelStateType, load_task_specific_weights: bool, quiet: bool = False) -> None`:\n",
    "  These two methods should be sufficient to get the current state of the model (e.g., parameters of the model) and reset to it.\n",
    "  This is used to retrieve the best state found during the training loop and restoring it when evaluating on the test set.\n",
    "  `load_task_specific_weights` is used to signal that only generic parameters of a model should be loaded, e.g., when preparing to fine-tune on a new task.\n",
    "\n",
    "* `is_param_task_specific(self, param_name: str) -> bool`:\n",
    "  This method is used to determine which parameters are task-specific to implement different learning rates for different parts of the model during fine-tuning.\n",
    "\n",
    "* `build_from_model_file(...) -> AbstractTorchFSMolModel[BatchFeaturesType]`:\n",
    "  This is the core factory method creating a fresh model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an `AbstractTorchFSMolModel` model\n",
    "\n",
    "We provide a default training loop for implementations of `AbstractTorchFSMolModel`, implemented as `train_loop` in `fs_mol.models.abstract_torch_fsmol_model`. It is in particular useful for fine-tuning a pre-trained model, but can be used to train full models as well.\n",
    "\n",
    "To use it, we need to assemble a number of components:\n",
    "* An actual model, implementing `AbstractTorchFSMolModel`.\n",
    "* A data pipeline providing appropriate minibatches to the model.\n",
    "* A validation function which will evaluate the model on the validation tasks during training.\n",
    "\n",
    "We will show these steps in detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the `GNNMultitask` model\n",
    "\n",
    "We first create the actual model - this is very much like any other PyTorch model, just requiring the interface discussed at the top of this notebook. We do not go into the details of the `GNNMultitask` model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fs_mol.models.gnn_multitask import (\n",
    "    GNNMultitaskConfig,\n",
    "    GNNMultitaskModel,\n",
    "    GNNConfig,\n",
    "    create_model,\n",
    ")\n",
    "from fs_mol.data.fsmol_dataset import FSMolDataset, DataFold\n",
    "\n",
    "fsmol_dataset = FSMolDataset.from_directory(FS_MOL_DATASET_PATH)\n",
    "\n",
    "# Set up an output directory in which to save a model\n",
    "out_dir = os.path.join(os.getcwd(), \"test\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Set up the model configuration that specifies a GNNMultitaskModel, using mostly default parameters.\n",
    "# Consult fs_mol/models/gnn_multitask.py for a full list.\n",
    "model_config = GNNMultitaskConfig(\n",
    "    num_tasks=fsmol_dataset.get_num_fold_tasks(DataFold.TRAIN),\n",
    "    gnn_config=GNNConfig(\n",
    "        type=\"PNA\",\n",
    "        hidden_dim=128,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# create an instance of a model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = create_model(model_config, device=device)\n",
    "\n",
    "print(f\"Num parameters {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"Model:\\n{model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Batching\n",
    "\n",
    "To train the model, we first need to set up a data pipeline, which we can do using our existing batching infrastructure (see notebooks/dataset.ipynb for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.data.multitask import MultitaskTaskSampleBatchIterable\n",
    "\n",
    "# note we need this dictionary to connect task numbers with names\n",
    "train_task_name_to_id = {\n",
    "    name: i for i, name in enumerate(fsmol_dataset.get_task_names(data_fold=DataFold.TRAIN))\n",
    "}\n",
    "\n",
    "train_data = MultitaskTaskSampleBatchIterable(\n",
    "    fsmol_dataset,\n",
    "    data_fold=DataFold.TRAIN,\n",
    "    task_name_to_id=train_task_name_to_id,\n",
    "    max_num_graphs=256,\n",
    ")\n",
    "print(f\"Have {len(train_task_name_to_id)} training tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Validation Function\n",
    "\n",
    "Our generic `train_loop` takes a function to evaluate the current model to determine when to stop training (using early stopping) and to identify the best checkpoint.\n",
    "\n",
    "While there are many potential validation functions, a particularly obvious one is to evaluate how well the model performs on the validation tasks when fine-tuned on them, for which we provide a `eval_model_by_finetuning_on_task` function. Internally, that function will kick off another `train_loop` on a single task sample and report the results of the fine-tuned model on the test of that task sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "from fs_mol.data import FSMolTaskSample\n",
    "from fs_mol.data.multitask import get_multitask_inference_batcher\n",
    "from fs_mol.models.abstract_torch_fsmol_model import save_model, eval_model_by_finetuning_on_task\n",
    "from fs_mol.utils.metrics import BinaryEvalMetrics\n",
    "from fs_mol.utils.test_utils import eval_model\n",
    "\n",
    "def validate_by_finetuning_on_tasks(\n",
    "    model: GNNMultitaskModel,\n",
    "    seed: int = 0,\n",
    ") -> float:\n",
    "    with tempfile.TemporaryDirectory() as tempdir:\n",
    "        # First, store the current state of the model, so that we can just load it back in\n",
    "        # repeatedly as starting point during finetuning:\n",
    "        current_model_path = os.path.join(tempdir, \"cur_model.pt\")\n",
    "        save_model(current_model_path, model)\n",
    "\n",
    "        # Move model off GPU to make space for validation model:\n",
    "        model_device = model.device\n",
    "        model = model.to(torch.device(\"cpu\"))\n",
    "\n",
    "        def test_model_fn(\n",
    "            task_sample: FSMolTaskSample, temp_out_folder: str, seed: int\n",
    "        ) -> BinaryEvalMetrics:\n",
    "            return eval_model_by_finetuning_on_task(\n",
    "                current_model_path,\n",
    "                model_cls=GNNMultitaskModel,\n",
    "                task_sample=task_sample,\n",
    "                batcher=get_multitask_inference_batcher(max_num_graphs=256),\n",
    "                learning_rate=0.00005,\n",
    "                task_specific_learning_rate=0.0001,\n",
    "                quiet=True,\n",
    "                device=model_device,\n",
    "            )\n",
    "\n",
    "        task_to_results = eval_model(\n",
    "            test_model_fn=test_model_fn,\n",
    "            dataset=fsmol_dataset,\n",
    "            train_set_sample_sizes=[16],\n",
    "            num_samples=1,\n",
    "            valid_size_or_ratio=0.2,\n",
    "            test_size_or_ratio=128,\n",
    "            fold=DataFold.VALIDATION,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        model = model.to(model_device)\n",
    "\n",
    "        # Compute mean of average precisions per task:\n",
    "        return np.mean(\n",
    "            [\n",
    "                np.mean([task_result.avg_precision for task_result in task_results])\n",
    "                for task_results in task_to_results.values()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training\n",
    "\n",
    "We are now ready to run training combining all of our components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.models.abstract_torch_fsmol_model import (\n",
    "    train_loop,\n",
    "    create_optimizer,\n",
    ")\n",
    "\n",
    "validate_by_finetuning_on_tasks(model)\n",
    "\n",
    "# create a specific optimizer with learning rate for training\n",
    "optimizer, lr_scheduler = create_optimizer(model, lr=0.00005, task_specific_lr=0.0001)\n",
    "\n",
    "# run a training loop on the model for a single epoch - this will take ~15 minutes on a P100, or very long a CPU-only machine.\n",
    "best_valid_metric, best_model_state = train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    train_data=train_data,\n",
    "    valid_fn=validate_by_finetuning_on_tasks,\n",
    "    max_num_epochs=1,\n",
    ")\n",
    "\n",
    "# restore best model parameters:\n",
    "model.load_model_state(best_model_state, load_task_specific_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "Finally, we can use the generic evaluation infrastructure (see notebooks/evaluation.ipynb) to evaluate the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from fs_mol.models.abstract_torch_fsmol_model import save_model\n",
    "\n",
    "with tempfile.TemporaryDirectory() as temp_dir:\n",
    "    model_weights_file = os.path.join(temp_dir, \"model.pt\")\n",
    "    save_model(model_weights_file, model)\n",
    "\n",
    "    def test_model_fn(\n",
    "        task_sample: FSMolTaskSample, temp_out_folder: str, seed: int\n",
    "    ) -> BinaryEvalMetrics:\n",
    "        return eval_model_by_finetuning_on_task(\n",
    "            model_weights_file,\n",
    "            model_cls=GNNMultitaskModel,\n",
    "            task_sample=task_sample,\n",
    "            batcher=get_multitask_inference_batcher(max_num_graphs=256),\n",
    "            learning_rate=0.00005,\n",
    "            task_specific_learning_rate=0.0001,\n",
    "            seed=seed,\n",
    "            quiet=True,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    eval_results = eval_model(\n",
    "        test_model_fn=test_model_fn,\n",
    "        dataset=fsmol_dataset,\n",
    "        # Require a validation set so that fine-tuning can work\n",
    "        valid_size_or_ratio=0.2,\n",
    "        # Restrict number of samples to one per task:\n",
    "        train_set_sample_sizes=[16],\n",
    "        num_samples=1,\n",
    "    )\n",
    "\n",
    "eval_results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91c2cdb63b030871da94aa046e171a8c212268d3e9d71e3496f8c89eaedb0da0"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
