{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the FSMol Dataset\n",
    "\n",
    "The `FSMolDataset` class provides access to the train/valid/test tasks of our few-shot learning dataset on molecules.\n",
    "\n",
    "An instance is created from the data directory by `FSMolDataset.from_directory(/path/to/dataset)`.\n",
    "The data in the `train`, `validation` and `test` folds of the dataset can be accessed using `FSMolDataset.get_task_reading_iterable`.\n",
    "By default, this method simply reads in the individual data files using a number of background worker processes and provides them in a standard format, but this behavior can be customized by providing a specialized callback function.\n",
    "The default implementation returns an iterable over `FSMolTask` objects, each containing an entire task's of single featurised molecules, `MoleculeDatapoint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up local details:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# This should be the location of the checkout of the FS-Mol repository:\n",
    "FS_MOL_CHECKOUT_PATH = os.path.join(os.environ['HOME'], \"Projects\", \"FS-Mol\")\n",
    "FS_MOL_DATASET_PATH = os.path.join(os.environ['HOME'], \"Projects\", \"FS-Mol\", \"datasets\")\n",
    "\n",
    "os.chdir(FS_MOL_CHECKOUT_PATH)\n",
    "sys.path.insert(0, FS_MOL_CHECKOUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Creating FSMolDataset and Accessing a Task\n",
    "\n",
    "First, we simply create a dataset and have a look at a single task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.data import FSMolDataset, DataFold\n",
    "dataset = FSMolDataset.from_directory(FS_MOL_DATASET_PATH)\n",
    "\n",
    "valid_task_iterable = dataset.get_task_reading_iterable(DataFold.VALIDATION)\n",
    "task = next(iter(valid_task_iterable))\n",
    "print(task.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A task, by default, is simply a name and a list of datapoints, each stored as a `MoleculeDatapoint`, a dataclass with the following definition:\n",
    "\n",
    "```python\n",
    "@dataclass(frozen=True)\n",
    "class MoleculeDatapoint:\n",
    "    \"\"\"Data structure holding information for a single molecule.\n",
    "\n",
    "    Args:\n",
    "        task_name: String describing the task this datapoint is taken from.\n",
    "        smiles: SMILES string describing the molecule this datapoint corresponds to.\n",
    "        graph: GraphData object containing information about the molecule in graph representation\n",
    "            form, according to featurization chosen in preprocessing.\n",
    "        numeric_label: numerical label (e.g., activity), usually measured in the lab\n",
    "        bool_label: bool classification label, usually derived from the numeric label using a\n",
    "            threshold.\n",
    "        fingerprint: optional ECFP for the molecule.\n",
    "        descriptors: optional phys-chem descriptors for the molecule.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: str\n",
    "    smiles: str\n",
    "    graph: GraphData\n",
    "    numeric_label: float\n",
    "    bool_label: bool\n",
    "    fingerprint: Optional[np.ndarray]\n",
    "    descriptors: Optional[np.ndarray]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.samples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Making a Task Sample\n",
    "\n",
    "In practice, all methods require that we sample from the `FSMolTask`s, for example during evaluation.\n",
    "To this end, we have implemented a number of samplers, extensions of the `TaskSampler` abstract class, which provides a unified `sample` method.\n",
    "The baseline models implemented in this repository use stratified sampling (implemented in `StratifiedTaskSampler`), but `RandomTaskSampler` and `BalancedTaskSampler` exist as alternatives.\n",
    "\n",
    "In practice, sampling often requires additional parameters, such as a minimal support set size, a minimal query set size, or an upper desired query set size. When a task dataset cannot be sampled using these parameters (e.g., because it's too small), a `SamplingException` exception is thrown and the caller can decide how to handle this. For example, during training, it is often reasonable to simply ignore such exceptions and pass over failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.data import StratifiedTaskSampler\n",
    "task_sampler = StratifiedTaskSampler(\n",
    "    train_size_or_ratio = 16,\n",
    "    valid_size_or_ratio = 0.0,\n",
    "    test_size_or_ratio = 256, \n",
    "    allow_smaller_test = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying a sampler returns a sample from the task, which contains a support/train set, validation set, and query/test set. In this case, the task is too small to return all requested testing samples, so it returns the maximum available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_sample = task_sampler.sample(task, seed=0)\n",
    "\n",
    "print(f\"Number of samples in task: {len(task.samples)}\")\n",
    "print(f\"Number of train samples: {len(task_sample.train_samples)}\")\n",
    "print(f\"Number of test samples: {len(task_sample.test_samples)}\")\n",
    "print(f\"Number of valid samples: {len(task_sample.valid_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samplers are built such that using the same seed will always return the same data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_sample_0 = task_sampler.sample(task, seed=0)\n",
    "task_sample_1 = task_sampler.sample(task, seed=1)\n",
    "\n",
    "print(f\"Task sample, seed 0, first call  - First training SMILES {task_sample.train_samples[0].smiles}\")\n",
    "print(f\"Task sample, seed 0, second call - First training SMILES {task_sample_0.train_samples[0].smiles}\")\n",
    "print(f\"Task sample, seed 1, first call  - First training SMILES {task_sample_1.train_samples[0].smiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Custom Task Reading Functions in MAML\n",
    "\n",
    "When implementing models on top of FS-Mol, it is often useful to specialize the callback used in `get_task_reading_iterable`, for example to directly draw appropriate samples or do further featurization.\n",
    "\n",
    "In MAML, we for example need to use stratified samples in each training step:\n",
    "\n",
    "```python\n",
    "task_sampler = StratifiedTaskSampler(\n",
    "    train_size_or_ratio=train_size,\n",
    "    valid_size_or_ratio=0,\n",
    "    test_size_or_ratio=(min_test_size, test_size),\n",
    ")\n",
    "\n",
    "def read_and_sample_from_task(paths: List[RichPath], id: int) -> Iterable[FSMolTaskSample]:\n",
    "    for i, path in enumerate(paths):\n",
    "        task = FSMolTask.load_from_file(path)\n",
    "        yield task_sampler.sample(task, seed=id + i)\n",
    "\n",
    "train_task_samples = dataset.get_task_reading_iterable(\n",
    "    data_fold=DataFold.TRAIN, task_reader_fn=read_and_sample_from_task\n",
    ")\n",
    "```\n",
    "\n",
    "In this instance, `train_task_samples` will now be an `Iterable` of sampled tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching Task Samples\n",
    "\n",
    "The `fs_mol.data` package also provides infrastructure for minibatching, using the `FSMolBatcher` class.\n",
    "These are framework-agnostic, and are used both for TensorFlow (in our MAML baseline) and Torch (in our MAT and Multitask baselines).\n",
    "Concretely, an `FSMolBatcher` object can be used to turn a list of datapoints into a sequence of minibatches.\n",
    "Our graph models handle batches of graphs as a single graph in which the samples appear as disconnected components. This is already implemented by our default implementation of `FSMolBatcher`, and so consumers only need to provide thing extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: GNN Multitask Batching\n",
    "\n",
    "As example, consider the GNN Multitask model (see `fs_mol/data/multitask.py` for full code), which needs to include the task ID for each sample. To do this, we extend the `FSMolBatcher` class using the callback hooks for initializing, extending and finalizing a batch:\n",
    "\n",
    "```python\n",
    "def multitask_batcher_init_fn(batch_data: Dict[str, Any]):\n",
    "    batch_data[\"sample_to_task_id\"] = []\n",
    "\n",
    "def multitask_batcher_add_sample_fn(\n",
    "    batch_data: Dict[str, Any],\n",
    "    sample_id: int,\n",
    "    sample: MoleculeDatapoint,\n",
    "    task_name_to_id: Dict[str, int],\n",
    "):\n",
    "    batch_data[\"sample_to_task_id\"].append(task_name_to_id[sample.task_name])\n",
    "\n",
    "def multitask_batcher_finalizer_fn(\n",
    "    batch_data: Dict[str, Any]\n",
    ") -> Tuple[FSMolMultitaskBatch, np.ndarray]:\n",
    "    plain_batch = fsmol_batch_finalizer(batch_data)\n",
    "    return (\n",
    "        FSMolMultitaskBatch(\n",
    "            sample_to_task_id=np.stack(batch_data[\"sample_to_task_id\"], axis=0),\n",
    "            **dataclasses.asdict(plain_batch),\n",
    "        ),\n",
    "        np.stack(batch_data[\"bool_labels\"], axis=0),\n",
    "    )\n",
    "\n",
    "def get_multitask_batcher(\n",
    "    task_name_to_id: Dict[str, int],\n",
    "    max_num_graphs: Optional[int] = None,\n",
    "    max_num_nodes: Optional[int] = None,\n",
    "    max_num_edges: Optional[int] = None,\n",
    ") -> FSMolBatcher[FSMolMultitaskBatch, np.ndarray]:\n",
    "    return FSMolBatcher(\n",
    "        max_num_graphs=max_num_graphs,\n",
    "        max_num_nodes=max_num_nodes,\n",
    "        max_num_edges=max_num_edges,\n",
    "        init_callback=multitask_batcher_init_fn,\n",
    "        per_datapoint_callback=partial(\n",
    "            multitask_batcher_add_sample_fn, task_name_to_id=task_name_to_id\n",
    "        ),\n",
    "        finalizer_callback=multitask_batcher_finalizer_fn,\n",
    "    )\n",
    "```\n",
    "\n",
    "This batcher is then used to create batches of up to `num_chunked_tasks` loaded in parallel as follows:\n",
    "\n",
    "```python\n",
    "def paths_to_mixed_samples(\n",
    "    paths: List[RichPath], idx: int\n",
    ") -> Iterable[Tuple[FSMolMultitaskBatch, np.ndarray]]:\n",
    "    loaded_samples: List[MoleculeDatapoint] = []\n",
    "    for i, path in enumerate(paths):\n",
    "        task = FSMolTask.load_from_file(path)\n",
    "        task_sample = self._task_sampler.sample(task, seed=idx + i)\n",
    "        loaded_samples.extend(task_sample.train_samples)\n",
    "    if self._data_fold == DataFold.TRAIN:\n",
    "        np.random.shuffle(loaded_samples)\n",
    "\n",
    "    for features, labels in self._batcher.batch(loaded_samples):\n",
    "        yield features, labels\n",
    "\n",
    "task_iterable = self._dataset.get_task_reading_iterable(\n",
    "    data_fold=self._data_fold,\n",
    "    task_reader_fn=paths_to_mixed_samples,\n",
    "    reader_chunk_size=self._num_chunked_tasks,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: MAML Batching\n",
    "\n",
    "We can similarly use this from a Tensorflow model, namely our MAML implementation (see `fs_mol/data/maml.py`). Here, we created a `TFGraphBatchIterable` class that uses a `FSMolBatcher` using a customized `finalizer_callback` to produce a batch suitable for consumption in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fs_mol.data.maml import TFGraphBatchIterable\n",
    "batched_data = TFGraphBatchIterable(\n",
    "    samples=task_sample.train_samples,\n",
    "    shuffle=True,\n",
    "    max_num_nodes=100,\n",
    ")\n",
    "\n",
    "print(next(iter(batched_data)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e06fcc65451699fab52210cecc89ce74d347871d8379f3a65371b5502fcda228"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
